{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "  Feature Norm_Contribution(Abs) Feature Norm_Contribution(Abs) Feature  \\\n",
      "0     V14               0.967066     V14               0.778443     V14   \n",
      "1     V26              0.0209581     V17               0.191617  Amount   \n",
      "2     V27             0.00598802  Amount              0.0239521     V24   \n",
      "3     V23             0.00299401     V28             0.00299401     V18   \n",
      "4     V28             0.00299401     V27             0.00299401     V22   \n",
      "\n",
      "  Norm_Contribution(Abs) Feature Norm_Contribution(Abs) Feature  \\\n",
      "0               0.943114     V17               0.571856     V14   \n",
      "1              0.0239521     V14               0.377246     V17   \n",
      "2              0.0209581  Amount              0.0299401  Amount   \n",
      "3             0.00598802     V27              0.0179641     V26   \n",
      "4             0.00299401     V26             0.00299401      V2   \n",
      "\n",
      "  Norm_Contribution(Abs)  ... Feature Norm_Contribution(Abs) Feature  \\\n",
      "0               0.949102  ...     V14               0.949102     V14   \n",
      "1              0.0269461  ...     V17              0.0239521     V17   \n",
      "2              0.0149701  ...  Amount              0.0239521  Amount   \n",
      "3             0.00898204  ...     V28             0.00299401     V26   \n",
      "4                      0  ...      V2                      0     V27   \n",
      "\n",
      "  Norm_Contribution(Abs) Feature Norm_Contribution(Abs) Feature  \\\n",
      "0               0.946108     V14               0.937126     V14   \n",
      "1              0.0239521     V24              0.0299401     V17   \n",
      "2              0.0149701     V22               0.011976     V26   \n",
      "3             0.00898204     V23             0.00898204     V24   \n",
      "4             0.00299401     V26             0.00299401  Amount   \n",
      "\n",
      "  Norm_Contribution(Abs) Feature Norm_Contribution(Abs)  \n",
      "0               0.949102     V17               0.502994  \n",
      "1              0.0269461     V14               0.449102  \n",
      "2               0.011976  Amount              0.0359281  \n",
      "3             0.00898204     V27             0.00898204  \n",
      "4             0.00299401     V28             0.00299401  \n",
      "\n",
      "[5 rows x 300 columns]\n",
      "CPU times: user 11d 13h 1min 22s, sys: 6h 12min 9s, total: 11d 19h 13min 32s\n",
      "Wall time: 2d 1h 18min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for non fraud observations  with step - up approach\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyBreakDown.explainer import Explainer\n",
    "from pyBreakDown.explanation import Explanation\n",
    "\n",
    "loaded_class0x = pickle.load(open('./Data/PickledData300/class0X.pickle', 'rb'))\n",
    "loaded_RFmodel = pickle.load(open('./SavedRFModel/Python_RF.pickle', 'rb'))\n",
    "loaded_exp = pickle.load(open('./Data/SavedExplainers/pybreakdownExplainer.pkl', 'rb'))\n",
    "\n",
    "columnnames = np.array(loaded_class0x.columns)\n",
    "datatopyBreak = loaded_class0x[0:150]\n",
    "\n",
    "Result = pd.DataFrame()\n",
    "R_norm = pd.DataFrame()\n",
    "R_norm_TOP5 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(datatopyBreak)):\n",
    "    print(i)\n",
    "    explanationu = loaded_exp.explain(observation=loaded_class0x.iloc[i],direction=\"down\")\n",
    "    pyBru = explanationu._attributes\n",
    "    pyBru = pd.DataFrame(pyBru, columns = ['Feature', 'Value', 'Contribution', 'Cumulative'])\n",
    "    pyBru = pyBru.drop(['Value', 'Cumulative'], axis=1)\n",
    "    pyBru = pyBru.tail(-1)\n",
    "    pyBru_array = pyBru.to_numpy()\n",
    "    pyBru_sortArray = pyBru_array[pyBru_array[:,1].argsort()]\n",
    "    pyBru_sortedArray = pyBru_sortArray[::-1]\n",
    "    df = pd.DataFrame(data=pyBru_sortedArray, columns=[\"Feature\", \"Contribution\"])\n",
    "    Result = pd.concat([Result, df], axis=1)\n",
    "    \n",
    "    #take absolute of contribution column and sum, then divide each row(abs) by the sum.\n",
    "    contrAbs = df['Contribution'].abs()\n",
    "    df['Abs_contribution'] = contrAbs\n",
    "    absSum = df['Abs_contribution'].sum()\n",
    "    #New_Norm_Contribution column sum up to 1\n",
    "    df['New_Norm_Contribution'] = df['Abs_contribution']/absSum\n",
    "    df_norm = df[['Feature','New_Norm_Contribution']]\n",
    "    \n",
    "    df_norm = df_norm.to_numpy()\n",
    "    df_norm = df_norm[df_norm[:,1].argsort()]\n",
    "    df_norm = df_norm[::-1]\n",
    "    df_norm = pd.DataFrame(data=df_norm, columns=[\"Feature\", \"Norm_Contribution(Abs)\"])\n",
    "    \n",
    "    R_norm = pd.concat([R_norm, df_norm], axis=1)\n",
    "    R_norm_TOP5 = R_norm.head(5)\n",
    "    \n",
    "#Result.to_csv('pyBDown_nonfraud_originalweights.csv', index = False, header=True, sep = \"\\t\")\n",
    "R_norm.to_csv('pyBDown_ALL_NORM_nonfraud.csv', index = False, header=True, sep = \"\\t\")\n",
    "#R_norm_TOP5.to_csv('pyBDown_NORM_TOP5_nonfraud.csv', index = False, header=True, sep = \"\\t\")\n",
    "print(R_norm_TOP5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "  Feature Norm_Contribution(Abs) Feature Norm_Contribution(Abs) Feature  \\\n",
      "0     V14               0.967066      V4               0.916168     V14   \n",
      "1      V3              0.0269461      V3              0.0838323      V4   \n",
      "2      V7             0.00598802     V14                      0     V13   \n",
      "3     V13                      0     V28                      0     V28   \n",
      "4     V28                      0      V1                      0      V1   \n",
      "\n",
      "  Norm_Contribution(Abs) Feature Norm_Contribution(Abs) Feature  \\\n",
      "0               0.943114      V3               0.916168     V14   \n",
      "1              0.0568862      V4              0.0838323      V3   \n",
      "2                      0     V14                      0      V2   \n",
      "3                      0     V28                      0     V13   \n",
      "4                      0      V1                      0     V28   \n",
      "\n",
      "  Norm_Contribution(Abs)  ... Feature Norm_Contribution(Abs) Feature  \\\n",
      "0               0.949102  ...     V14               0.949102     V14   \n",
      "1              0.0449102  ...      V3              0.0419162     V12   \n",
      "2             0.00598802  ...      V1             0.00898204      V3   \n",
      "3                      0  ...     V13                      0     V13   \n",
      "4                      0  ...     V28                      0     V28   \n",
      "\n",
      "  Norm_Contribution(Abs) Feature Norm_Contribution(Abs) Feature  \\\n",
      "0               0.946108     V12               0.937126     V14   \n",
      "1              0.0449102      V4              0.0538922      V4   \n",
      "2             0.00898204      V3             0.00598802     V13   \n",
      "3                      0      V5             0.00299401     V12   \n",
      "4                      0     V28                      0     V28   \n",
      "\n",
      "  Norm_Contribution(Abs) Feature Norm_Contribution(Abs)  \n",
      "0               0.949102     V12               0.721557  \n",
      "1              0.0449102      V4               0.269461  \n",
      "2             0.00598802      V1             0.00898204  \n",
      "3                      0     V14                      0  \n",
      "4                      0     V28                      0  \n",
      "\n",
      "[5 rows x 300 columns]\n",
      "CPU times: user 9d 2h 8min 55s, sys: 4h 31min 36s, total: 9d 6h 40min 32s\n",
      "Wall time: 1d 6h 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  for non fraud observations  with step - down approach\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyBreakDown.explainer import Explainer\n",
    "from pyBreakDown.explanation import Explanation\n",
    "\n",
    "loaded_class0x = pickle.load(open('./Data/PickledData300/class0X.pickle', 'rb'))\n",
    "loaded_RFmodel = pickle.load(open('./SavedRFModel/Python_RF.pickle', 'rb'))\n",
    "loaded_exp = pickle.load(open('./SavedExplainers/pybreakdownExplainer.pkl', 'rb'))\n",
    "\n",
    "columnnames = np.array(loaded_class0x.columns)\n",
    "datatopyBreak = loaded_class0x[0:150]\n",
    "\n",
    "Result = pd.DataFrame()\n",
    "R_norm = pd.DataFrame()\n",
    "R_norm_TOP5 = pd.DataFrame()\n",
    "\n",
    "for i in range(len(datatopyBreak)):\n",
    "    print(i)\n",
    "    explanationu = loaded_exp.explain(observation=loaded_class0x.iloc[i],direction=\"up\")\n",
    "    pyBru = explanationu._attributes\n",
    "    pyBru = pd.DataFrame(pyBru, columns = ['Feature', 'Value', 'Contribution', 'Cumulative'])\n",
    "    pyBru = pyBru.drop(['Value', 'Cumulative'], axis=1)\n",
    "    pyBru = pyBru.tail(-1)\n",
    "    pyBru_array = pyBru.to_numpy()\n",
    "    pyBru_sortArray = pyBru_array[pyBru_array[:,1].argsort()]\n",
    "    pyBru_sortedArray = pyBru_sortArray[::-1]\n",
    "    df = pd.DataFrame(data=pyBru_sortedArray, columns=[\"Feature\", \"Contribution\"])\n",
    "    Result = pd.concat([Result, df], axis=1)\n",
    "    \n",
    "    #take absolute of contribution column and sum, then divide each row(abs) by the sum.\n",
    "    contrAbs = df['Contribution'].abs()\n",
    "    df['Abs_contribution'] = contrAbs\n",
    "    absSum = df['Abs_contribution'].sum()\n",
    "    #New_Norm_Contribution column sum up to 1\n",
    "    df['New_Norm_Contribution'] = df['Abs_contribution']/absSum\n",
    "    df_norm = df[['Feature','New_Norm_Contribution']]\n",
    "    \n",
    "    df_norm = df_norm.to_numpy()\n",
    "    df_norm = df_norm[df_norm[:,1].argsort()]\n",
    "    df_norm = df_norm[::-1]\n",
    "    df_norm = pd.DataFrame(data=df_norm, columns=[\"Feature\", \"Norm_Contribution(Abs)\"])\n",
    "    \n",
    "    R_norm = pd.concat([R_norm, df_norm], axis=1)\n",
    "    R_norm_TOP5 = R_norm.head(5)\n",
    "    \n",
    "#Result.to_csv('pyBUp_nonfraud_originalweights.csv', index = False, header=True, sep = \"\\t\")\n",
    "R_norm.to_csv('pyBUp_ALL_NORM_nonfraud.csv', index = False, header=True, sep = \"\\t\")\n",
    "#R_norm_TOP5.to_csv('pyBUp_NORM_TOP5_nonfraud.csv', index = False, header=True, sep = \"\\t\")\n",
    "print(R_norm_TOP5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
